<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>When you first decide to build a deep learning framework, you immediately hit a fundamental fork in the road: how â€œsmartâ€ should your main <code class="language-plaintext highlighter-rouge">NDArray</code> object be?</p> <p>On one side, you have the â€œthinâ€ wrapper. This approach is tempting. The <code class="language-plaintext highlighter-rouge">NDArray</code> class is just a simple shell, and it delegates <em>everything</em>â€”all the math, all the striding logic, all the broadcastingâ€”to its backend (NumPy, C++, Metal). This sounds clean, but itâ€™s a maintenance nightmare. It means you have to re-implement all that complex, error-prone view logic in C++, then <em>again</em> in Metal, and <em>again</em> in CUDA. This design doesnâ€™t scale.</p> <p>Then, thereâ€™s the â€œthickâ€ wrapper. This is the design Iâ€™m building, and itâ€™s built on a clean separation of concerns I call the <strong>Control Plane vs. Data Plane</strong> model.</p> <h3 id="ï¸-the-control-plane-a-smart-python-wrapper">âœˆï¸ The Control Plane: A â€œSmartâ€ Python Wrapper</h3> <p>The â€œControl Planeâ€ is my Python <code class="language-plaintext highlighter-rouge">NDArray</code> class. Itâ€™s â€œthickâ€ because it handles <strong>all the logical operations</strong> of the array.</p> <p>At its core, my <code class="language-plaintext highlighter-rouge">NDArray</code> is just a map. Itâ€™s a Python object that holds metadataâ€”<code class="language-plaintext highlighter-rouge">shape</code>, <code class="language-plaintext highlighter-rouge">strides</code>, and <code class="language-plaintext highlighter-rouge">offset</code>â€”which defines a â€œlogical viewâ€ over a â€œphysicalâ€ block of memory (which I call the <code class="language-plaintext highlighter-rouge">_handle</code>).</p> <p>The key insight is that <strong>a huge number of array operations are just math on this metadata.</strong> They donâ€™t need to touch the data at all, making them zero-copy, free operations.</p> <ul> <li> <strong><code class="language-plaintext highlighter-rouge">transpose()</code> or <code class="language-plaintext highlighter-rouge">permute()</code>?</strong> Thatâ€™s not a computation. I just swap the numbers in the <code class="language-plaintext highlighter-rouge">shape</code> and <code class="language-plaintext highlighter-rouge">strides</code> tuples. Itâ€™s an O(1) operation.</li> <li> <strong>Slicing (<code class="language-plaintext highlighter-rouge">a[1:5, ::2]</code>)?</strong> Thatâ€™s just a math problem. I just calculate a new <code class="language-plaintext highlighter-rouge">offset</code> (to jump to the <code class="language-plaintext highlighter-rouge">[1, 0]</code> element) and new <code class="language-plaintext highlighter-rouge">strides</code> (to handle the <code class="language-plaintext highlighter-rouge">::2</code> step). Itâ€™s free.</li> <li> <strong><code class="language-plaintext highlighter-rouge">broadcast_to()</code>?</strong> Thatâ€™s the â€œzero-stride trick.â€ By setting the stride of a new dimension to <code class="language-plaintext highlighter-rouge">0</code>, I can â€œstretchâ€ an array from shape <code class="language-plaintext highlighter-rouge">(3,)</code> to <code class="language-plaintext highlighter-rouge">(10, 3)</code> without allocating any new memory.</li> </ul> <p>The Control Plane is where all this â€œviewâ€ magic lives. Itâ€™s written once, in pure Python, and itâ€™s easy to test and debug.</p> <h3 id="ï¸-the-data-plane-a-dumb-c-engine">âš™ï¸ The Data Plane: A â€œDumbâ€ C++ Engine</h3> <p>The â€œData Planeâ€ is my C++/Metal/CUDA backend. Itâ€™s the â€œmuscle.â€ It is designed to be <strong>brutally fast and incredibly simple.</strong></p> <p>My C++ backend doesnâ€™t know what a â€œstrideâ€ is. It doesnâ€™t know what â€œbroadcastingâ€ is. Itâ€™s a â€œdumbâ€ compute engine that expects one thing: <strong>a flat, 1D, contiguous block of memory.</strong></p> <p>Its entire API is just a set of C-style functions that operate on these flat buffers:</p> <ul> <li><code class="language-plaintext highlighter-rouge">ewise_add(a_handle, b_handle, out_handle)</code></li> <li><code class="language-plaintext highlighter-rouge">matmul(a_handle, b_handle, out_handle, M, K, N)</code></li> <li><code class="language-plaintext highlighter-rouge">reduce_sum(in_handle, out_handle, reduce_size)</code></li> </ul> <p>This makes my backend C++ code <em>radically</em> simpler. The <code class="language-plaintext highlighter-rouge">reduce_sum</code> kernel, for example, just loops over contiguous blocks of memory. It doesnâ€™t need to know <em>anything</em> about the original tensorâ€™s shape.</p> <p>Best of all, the API for the C++ backend is now <strong>identical</strong> to the API for the Metal backend, which will be identical to the API for the CUDA backend. Adding new hardware is now trivial.</p> <hr> <h3 id="-the-bridge-the-compact-function">ğŸŒ‰ The Bridge: The <code class="language-plaintext highlighter-rouge">compact()</code> Function</h3> <p>This leads to the obvious question: what happens when the â€œsmartâ€ Control Plane (with its fragmented, non-contiguous view) needs to talk to the â€œdumbâ€ Data Plane (which needs a flat array)?</p> <p>This is where the most important function in the library comes in: <code class="language-plaintext highlighter-rouge">compact()</code>.</p> <p>I call <code class="language-plaintext highlighter-rouge">compact()</code> the <strong>â€œView Tax.â€</strong></p> <p>Itâ€™s the (necessary) performance hit you take to connect the two planes. When I call <code class="language-plaintext highlighter-rouge">a.transpose() @ b</code>, the <code class="language-plaintext highlighter-rouge">matmul</code> operation does this:</p> <ol> <li> <strong>Check <code class="language-plaintext highlighter-rouge">a</code>:</strong> The Control Plane sees that <code class="language-plaintext highlighter-rouge">a</code> is a <code class="language-plaintext highlighter-rouge">transpose()</code> view. Its <code class="language-plaintext highlighter-rouge">strides</code> are <code class="language-plaintext highlighter-rouge">(1, 10)</code> instead of <code class="language-plaintext highlighter-rouge">(10, 1)</code>, so itâ€™s non-contiguous.</li> <li> <strong>Pay the Tax:</strong> It calls <code class="language-plaintext highlighter-rouge">a_compact = a.compact()</code>.</li> <li> <strong><code class="language-plaintext highlighter-rouge">compact()</code>:</strong> This function allocates a <em>new</em>, contiguous block of memory and (relatively slowly) copies the data from the â€œfragmentedâ€ view into this new, â€œdefragmentedâ€ buffer.</li> <li> <strong>Call the Data Plane:</strong> The <code class="language-plaintext highlighter-rouge">matmul</code> kernel is now called with the <em>new</em>, <em>compact</em> <code class="language-plaintext highlighter-rouge">a_compact._handle</code>. The C++ code doesnâ€™t have to deal with the <code class="language-plaintext highlighter-rouge">(1, 10)</code> strides; it just gets a simple, flat array and runs at maximum speed.</li> </ol> <p>This is the entire philosophy of the design. As I wrote in my original notes:</p> <blockquote> <p>We call <code class="language-plaintext highlighter-rouge">.compact()</code> (which copies memory) liberally in order to make the underlying C++ implementations simpler. Everything that can be done in Python, is done in Python.</p> </blockquote> <p>This trade-offâ€”accepting a few explicit data-copy â€œtaxesâ€ in exchange for a massive reduction in backend complexityâ€”is the key to building a system that is sane, maintainable, and scalable.</p> </body></html>