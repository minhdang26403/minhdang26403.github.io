<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://minhdang26403.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://minhdang26403.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-12T03:33:31+00:00</updated><id>https://minhdang26403.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">An Engineerâ€™s Guide to Deep Learning Optimizers</title><link href="https://minhdang26403.github.io/blog/2025/optimizers/" rel="alternate" type="text/html" title="An Engineerâ€™s Guide to Deep Learning Optimizers"/><published>2025-11-11T22:30:00+00:00</published><updated>2025-11-11T22:30:00+00:00</updated><id>https://minhdang26403.github.io/blog/2025/optimizers</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2025/optimizers/"><![CDATA[<h2 id="part-1-the-impossible-dream-and-the-noisy-workaround">Part 1: The Impossible Dream and the Noisy Workaround</h2> <h3 id="introduction-the-mountain-in-the-fog">Introduction: The Mountain in the Fog</h3> <p>If youâ€™re an engineer, youâ€™ve been trained to find the â€œbestâ€ solution. So when you start with deep learning, you hit a simple problem: we have a loss function, and we just want to find the lowest point. This is a solved problem, right?</p> <p>Well, no. The â€œloss landscapeâ€ of a neural network isnâ€™t a smooth, convex bowl. Itâ€™s a nightmarish, high-dimensional mountain range with a million peaks, valleys, and saddle points, and itâ€™s all shrouded in a dense fog.</p> <ul> <li><strong>High-Dimensional:</strong> You donâ€™t have two parameters (x, y) to tune. You have 175 billion.</li> <li><strong>Non-Convex:</strong> There are countless â€œlocal minimaâ€ (valleys) that <em>look</em> like the lowest point but arenâ€™t.</li> <li><strong>Stochastic:</strong> Youâ€™re in a fog. You canâ€™t even <em>see</em> the whole mountain range. You can only get a <em>noisy guess</em> of the slope from your immediate surroundings.</li> </ul> <p>This is the real job of an optimizer: not to find the â€œperfectâ€ global minimum, but to find <em>any</em> wide, flat, â€œgood enoughâ€ valley, as quickly as possible, without falling off a cliff.</p> <p>This is the story of how we built a better â€œhikerâ€ for this insane terrain. It starts with the textbook idea and its first, critical, systems-level failure.</p> <hr/> <h3 id="the-baseline-gradient-descent-gd">The Baseline: Gradient Descent (GD)</h3> <p>Gradient Descent (also called â€œBatchâ€ Gradient Descent) is the simple, textbook algorithm you learn in your first calculus class.</p> <p><strong>The Idea:</strong> â€œBefore I take a single step, I will poll <em>every single person</em> (data point) in the entire dataset. Iâ€™ll average all their opinions of which way is â€˜downhillâ€™ to get a perfect, noise-free gradient, and <em>then</em> Iâ€™ll take one, confident step in that exact direction.â€</p> <p>The update rule is just: $W_{\text{new}} = W_{\text{old}} - \eta \cdot \nabla L(W)$</p> <p>Where $\nabla L(W)$ is the gradient (derivative) of the loss, calculated over <strong>all</strong> training examples.</p> <p><strong>The â€œWhy Notâ€: The Systems-Level Dealbreaker</strong></p> <p>This sounds great, but itâ€™s a systems-level catastrophe.</p> <ul> <li><strong>Problem:</strong> You have a 10TB dataset of images.</li> <li><strong>The GD approach:</strong> You must run a full forward and backward pass over all <em>10 terabytes</em> of data just to compute a <em>single</em> gradient.</li> <li><strong>The Result:</strong> You will update your weights <em>once</em> every few hours. Your model will be â€œtrainingâ€ but going nowhere.</li> </ul> <p>Itâ€™s computationally pure, but practically impossible. The bottleneck isnâ€™t the math; itâ€™s the I/O and compute time of using the entire dataset for every single step.</p> <hr/> <h3 id="the-workaround-stochastic-gradient-descent-sgd">The Workaround: Stochastic Gradient Descent (SGD)</h3> <p>This is the first brilliant, practical â€œpatchâ€ that makes deep learning possible.</p> <p><strong>The Idea:</strong> â€œPolling the entire dataset is a waste of time. Iâ€™ll just grab a <em>small, random handful</em> (a â€œmini-batchâ€) of 64 data points, ask them which way is downhill, and take a quick, messy, â€˜good enoughâ€™ step in that direction.â€</p> <p><strong>The â€œWhyâ€: The Two Big Wins</strong></p> <p>SGD solves the first problem but introduces a new one (which weâ€™ll fix later).</p> <p><strong>1. Itâ€™s Fast. Mind-Bogglingly Fast.</strong> This is the main point. In the time it takes GD to compute <em>one</em> perfect step, SGD has already taken 10,000 â€œnoisyâ€ steps and is 90% of the way to the valley. Itâ€™s the difference between planning a perfect cross-country road trip and just getting in the car and driving west.</p> <p><strong>2. The â€œBugâ€ is a Feature: Noise is a Regularizer</strong> This is the non-obvious magic of SGD. The gradient from a mini-batch is <em>noisy</em>. Itâ€™s not the â€œtrueâ€ gradient. This means the optimizerâ€™s path is a chaotic, zig-zagging mess.</p> <p>It turns out this is exactly what we want.</p> <p>In a non-convex landscape, the â€œperfectâ€ minimum is often a very <em>sharp, narrow</em> valley. This is called <strong>overfitting</strong>â€”the model has perfectly memorized the training data but will fail on new data.</p> <p>The noise in SGD acts like a drunk hiker. Itâ€™s too shaky and chaotic to fall into those narrow, sharp valleys. It prefers to â€œbounce aroundâ€ until it settles into a <strong>wide, flat valley</strong>. A flat valley is a <em>generalizable</em> solutionâ€”it means that even if the new data is slightly different, the loss is still low.</p> <p><strong>The New Problem</strong></p> <p>But that â€œzig-zagâ€ is still a problem.</p> <ol> <li>If the valley is a steep, narrow ravine (like in the image above), SGD will spend all its time bouncing off the walls, making very slow progress down the <em>actual</em> slope.</li> <li>If the valley is a <em>very flat</em> plateau, the gradients are tiny, and the â€œnoisyâ€ steps are almost zero. SGD just sits there, barely moving.</li> </ol> <p>This is our next bug. We need a way to <strong>dampen the zig-zagging</strong> and <strong>build up speed (momentum)</strong> in consistent directions.</p> <p>â€¦and thatâ€™s exactly what our first â€œpatch,â€ SGD with Momentum, is designed to fix.</p> <h2 id="part-2-the-snowball-patch-sgd-with-momentum">Part 2: The Snowball Patch (SGD with Momentum)</h2> <h3 id="the-baseline-the-drunk-hiker-sgd">The Baseline: The â€œDrunk Hikerâ€ (SGD)</h3> <p>In Part 1, we established our baseline: Stochastic Gradient Descent (SGD). Itâ€™s the â€œdrunk hikerâ€ in the foggy, non-convex mountain range of our loss landscape.</p> <ul> <li><strong>The Good:</strong> Itâ€™s fast (uses mini-batches) and its â€œnoisyâ€ path is a feature, helping it bounce out of sharp, â€œoverfittingâ€ minima and find the wide, generalizable valleys.</li> <li><strong>The Bad:</strong> That same noise is a critical bug. It creates two huge problems: <ol> <li><strong>Oscillation:</strong> In a steep, narrow ravine, the hiker just bounces from wall to wall, making almost no <em>downhill</em> progress.</li> <li><strong>Stalling:</strong> On a long, flat plateau, the gradient (slope) is tiny. The hiker takes tiny, hesitant steps and barely moves at all, stalling the training.</li> </ol> </li> </ul> <p>We need a fix. We need to give our hiker a way to â€œaverage outâ€ the noise from the zig-zagging and â€œbuild up speedâ€ on the flats. This fix is called <strong>Momentum</strong>.</p> <hr/> <h3 id="the-first-fix-sgd-with-momentum-the-snowball">The First Fix: SGD with Momentum (The Snowball)</h3> <p><strong>The Idea:</strong> Instead of taking a step based <em>only</em> on the current (noisy) gradient, we take a step based on a <strong>moving average</strong> of all past gradients.</p> <p>Think of it as replacing our lightweight hiker with a heavy, unstoppable snowball.</p> <p>At each step, we do two things:</p> <ol> <li>We apply friction to the â€œsnowballâ€ (the old momentum), reducing its speed by a fraction (e.g., 10%).</li> <li>We add the new gradient (the â€œpushâ€ from the new mini-batch) to the snowball.</li> </ol> <p>In code, this â€œvelocityâ€ vector $v$ is updated like this (where $\beta$ is the momentum term, usually 0.9):</p> <p>$v_t = (\beta \cdot v_{t-1}) + g_t$ $W_{\text{new}} = W_{\text{old}} - \eta \cdot v_t$</p> <p>(Note: Youâ€™ll see different forms of this equation, but they all share this core idea: the current step is a combination of the previous step and the new gradient.)</p> <p>This one simple change brilliantly solves <em>both</em> of our problems.</p> <h4 id="1-the-fix-for-oscillation-the-smoother">1. The Fix for Oscillation: The â€œSmootherâ€</h4> <p>How does this stop the â€œzig-zagâ€?</p> <p>Imagine our optimizer is in that narrow ravine. The â€œbouncesâ€ are just noisy gradients pointing in opposite directions.</p> <ul> <li><strong>Step 1 Gradient ($g_1$):</strong> <code class="language-plaintext highlighter-rouge">[+10, -0.1]</code> (Bounces right, moves slightly down)</li> <li><strong>Step 2 Gradient ($g_2$):</strong> <code class="language-plaintext highlighter-rouge">[-10, -0.1]</code> (Bounces left, moves slightly down)</li> </ul> <p><strong>Vanilla SGD</strong> would just move right, then left, and barely go anywhere.</p> <p><strong>Momentum</strong> averages them:</p> <ul> <li><strong>Step 1 Velocity ($v_1$):</strong> <code class="language-plaintext highlighter-rouge">[+10, -0.1]</code></li> <li><strong>Step 2 Velocity ($v_2$):</strong> $(\beta \cdot v_1) + g_2$ <code class="language-plaintext highlighter-rouge">v_2 = (0.9 * [+10, -0.1]) + [-10, -0.1]</code> <code class="language-plaintext highlighter-rouge">v_2 = [+9, -0.09] + [-10, -0.1]</code> <code class="language-plaintext highlighter-rouge">v_2 = [-1, -0.19]</code></li> </ul> <p>Look at that! The horizontal part (the <code class="language-plaintext highlighter-rouge">+10</code> and <code class="language-plaintext highlighter-rouge">-10</code>) has <strong>averaged out</strong> and cancelled. The vertical part (the <code class="language-plaintext highlighter-rouge">-0.1</code> and <code class="language-plaintext highlighter-rouge">-0.1</code>) has <strong>accumulated</strong>.</p> <p>The snowballâ€™s momentum in the zig-zag directions dies, while its momentum in the consistent â€œdownhillâ€ direction builds. The path becomes dramatically smoother.</p> <h4 id="2-the-fix-for-stalling-the-accelerator">2. The Fix for Stalling: The â€œAcceleratorâ€</h4> <p>This is the â€œsnowballâ€ effect.</p> <p>Now, imagine our optimizer is on that long, flat plateau. The gradient is tiny but consistent.</p> <ul> <li><strong>Gradient at all steps:</strong> <code class="language-plaintext highlighter-rouge">[-0.01]</code> (a tiny, but consistent push)</li> </ul> <p><strong>Vanilla SGD</strong> would just take tiny steps: <code class="language-plaintext highlighter-rouge">-0.01</code>, <code class="language-plaintext highlighter-rouge">-0.01</code>, <code class="language-plaintext highlighter-rouge">-0.01</code>â€¦ It would take 100 steps just to move a total of 1.0.</p> <p><strong>Momentum</strong> builds up speed:</p> <ul> <li><strong>$v_1$:</strong> <code class="language-plaintext highlighter-rouge">-0.01</code></li> <li><strong>$v_2$:</strong> $(0.9 \cdot -0.01) + -0.01 = -0.019$</li> <li><strong>$v_3$:</strong> $(0.9 \cdot -0.019) + -0.01 = -0.0271$</li> <li><strong>$v_4$:</strong> $(0.9 \cdot -0.0271) + -0.01 = -0.03439$</li> </ul> <p>The velocity is <em>accelerating</em>. The snowball is picking up speed, allowing the optimizer to â€œshootâ€ across the flat plateau and converge much faster.</p> <hr/> <h3 id="the-new-problem">The New Problem</h3> <p>So, weâ€™re done, right? SGD with Momentum is great. Itâ€™s fast, and itâ€™s smooth.</p> <p>Weâ€™ve solved the big problems, but weâ€™ve exposed two new, more subtle ones.</p> <ol> <li> <p><strong>The â€œCold-Startâ€ Problem:</strong> What is the velocity $v_0$ at the very beginning of training? Itâ€™s <code class="language-plaintext highlighter-rouge">0</code>. Because our â€œsnowballâ€ starts with zero momentum, it takes several steps to â€œwarm upâ€ and get to a reasonable speed. This means the first few iterations are artificially slow and hesitant. This is the <strong>initialization bias</strong>.</p> </li> <li> <p><strong>The â€œOne-Size-Fits-Allâ€ Problem:</strong> We are still using <em>one</em> learning rate $\eta$ for all 175 billion parameters. This is a huge issue. What if the loss landscape for your <code class="language-plaintext highlighter-rouge">Weight_1</code> is a gentle, flat plain, but the landscape for <code class="language-plaintext highlighter-rouge">Weight_1000</code> is a treacherous, spiky mountain?</p> <ul> <li><code class="language-plaintext highlighter-rouge">Weight_1</code> needs a <em>big</em> learning rate to move faster.</li> <li><code class="language-plaintext highlighter-rouge">Weight_1000</code> needs a <em>tiny</em> learning rate to avoid exploding.</li> </ul> <p>Our current optimizer gives them both the <em>same</em> learning rate. Itâ€™s like forcing our downhill skier and our cross-country skier to use the same pair of skis.</p> </li> </ol> <p>To fix this, we need a way to give every parameter its <em>own</em>, unique, <strong>adaptive learning rate</strong>.</p> <p>And <em>that</em> brings us to the next generation of optimizersâ€¦ and the clever ideas that led to Adam.</p> <hr/> <h2 id="part-3-the-king-of-the-hill-adaptive-moment-estimation">Part 3: The King of the Hill (Adaptive Moment Estimation)</h2> <h3 id="the-story-so-far">The Story So Far</h3> <p>In Part 1, we established that â€œBatchâ€ Gradient Descent is impossible, so we use <strong>Stochastic Gradient Descent (SGD)</strong>. But SGD, our â€œdrunk hiker,â€ is noisy. It zig-zags in steep ravines and stalls on flat plains.</p> <p>In Part 2, we added the <strong>Momentum</strong> â€œpatch,â€ which turns our hiker into a heavy â€œsnowball.â€ This solves our two problems:</p> <ol> <li><strong>It smooths out</strong> the zig-zagging by averaging gradients.</li> <li><strong>It accelerates</strong> across flat plains by building up speed.</li> </ol> <p>But we were left with two new, more subtle bugs in our optimizer:</p> <ol> <li><strong>The â€œCold-Startâ€ Problem:</strong> Our snowball starts with zero velocity. It takes a bunch of steps to â€œwarm up,â€ making the start of training artificially slow. This is an <strong>initialization bias</strong>.</li> <li><strong>The â€œOne-Size-Fits-Allâ€ Problem:</strong> Weâ€™re still using <em>one</em> learning rate $\eta$ for all 175 billion parameters. This is a huge issue. The terrain for <code class="language-plaintext highlighter-rouge">Weight_1</code> might be a flat, gentle plain (needs a <em>big</em> step), while the terrain for <code class="language-plaintext highlighter-rouge">Weight_1000</code> is a spiky, treacherous mountain (needs a <em>tiny</em> step). Weâ€™re giving them both the same boot size.</li> </ol> <p>We need an optimizer that can fix <em>both</em>. We need a â€œsnowballâ€ that starts fast and has â€œadaptive skisâ€ that adjust to the terrain of each parameter.</p> <p>This brings us to <strong>Adam</strong>.</p> <hr/> <h3 id="the-final-patch-adam-adaptive-moment-estimation">The Final Patch: Adam (Adaptive Moment Estimation)</h3> <p>The name â€œAdamâ€ is a spoiler. It stands for <strong>Adaptive Moment Estimation</strong>. It doesnâ€™t just estimate the <em>first</em> moment (the â€œsnowballâ€ velocity). It <em>also</em> estimates the <em>second</em> moment (the â€œvolatilityâ€ of the gradient) and uses it to <em>adapt</em> the learning rate.</p> <p>Adam is not a new idea from scratch. Itâ€™s the â€œAvengers Assembleâ€ of optimizers, combining two best-in-class solutions into one algorithm.</p> <h4 id="the-momentum-fix-the-m-in-adam">The Momentum Fix (The â€œMâ€ in Adam)</h4> <p>This first part of Adam is just SGD with Momentum, but with a fix for the â€œcold-startâ€ problem.</p> <ul> <li><strong>The Biased Snowball ($m_t$):</strong> Adam keeps track of the momentum (the first moment) just like before: $m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$</li> <li><strong>The â€œCold-Startâ€ Bug:</strong> As we said, $m_0$ is initialized to 0. This makes $m_1$, $m_2$, $m_3$, etc., all artificially small. They are <em>biased</em> toward zero.</li> <li><strong>The Unbiasing Trick:</strong> The Adam authors provided a simple, brilliant fix. They calculate this bias analytically and just divide it out. $\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$</li> </ul> <p>Letâ€™s see this in action (with $\beta_1 = 0.9$):</p> <ul> <li><strong>At Step 1 ($t=1$):</strong> The denominator is $(1 - 0.9^1) = 0.1$. The biased $m_1$ is divided by 0.1 (multiplied by 10), perfectly correcting it.</li> <li><strong>At Step 500 ($t=500$):</strong> The denominator is $(1 - 0.9^{500})$, which is $\approx 1.0$. The correction <em>automatically fades away</em> and does nothing.</li> </ul> <p>This â€œbias-correctedâ€ momentum $\hat{m}_t$ is the first half of Adam. Itâ€™s a â€œsnowballâ€ that starts at full speed.</p> <h4 id="the-adaptive-fix-the-a-in-adam">The Adaptive Fix (The â€œAâ€ in Adam)</h4> <p>This is the second, more powerful idea. It solves the â€œone-size-fits-allâ€ problem.</p> <ul> <li><strong>The Problem:</strong> We need a <em>per-parameter</em> learning rate. We need to know if the terrain for <code class="language-plaintext highlighter-rouge">Weight_1</code> is flat or spiky.</li> <li><strong>The Fix (RMSprop):</strong> We can <em>measure</em> the â€œspikinessâ€ of the terrain by tracking a moving average of the <em>squared gradients</em>. This is the <strong>second moment ($v_t$)</strong>. $v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot (g_t)^2$</li> <li><strong>Why squared?</strong> Squaring $g_t$ makes it positive. Now, $v_t$ is a measure of â€œgradient volatility.â€ <ul> <li>If $v_t$ is <em>large</em>, it means this parameter has huge, spiky gradients. The terrain is treacherous.</li> <li>If $v_t$ is <em>small</em>, it means this parameter has tiny, consistent gradients. The terrain is flat.</li> </ul> </li> <li><strong>The â€œAha!â€ Moment:</strong> The final Adam update rule is, conceptually: $W_{\text{new}} = W_{\text{old}} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$</li> </ul> <p>Look at that division. We are dividing our â€œsnowballâ€ step ($\hat{m}_t$) by the square root of its â€œvolatilityâ€ ($\hat{v}_t$).</p> <p>This gives every single parameter its own learning rate:</p> <ul> <li><strong>Spiky Terrain (Large $v_t$):</strong> The denominator is <em>big</em>. The step size becomes <em>small</em>. Adam says, â€œWhoa, be careful on this parameter. Take tiny, safe steps.â€</li> <li><strong>Flat Terrain (Small $v_t$):</strong> The denominator is <em>tiny</em>. The step size is <em>amplified</em>. Adam says, â€œThis parameter is on a boring flat, letâ€™s give it a boost and go faster!â€</li> </ul> <p>(And yes, this $v_t$ term is <em>also</em> biased toward zero at the start, so Adam applies the <em>same</em> unbiasing trick to it, giving $\hat{v}_t$).</p> <hr/> <h3 id="conclusion-the-king-is-crowned">Conclusion: The King is Crowned</h3> <p>Adam is the default king because it combines both fixes.</p> <ul> <li>Itâ€™s a <strong>snowball</strong> (using momentum, $\hat{m}<em>t$) that knows which _direction</em> to go.</li> <li>It has <strong>adaptive skis</strong> (using $\sqrt{\hat{v}<em>t}$) that automatically adjust for the _terrain</em> of <em>every single parameter</em>.</li> <li>It has a <strong>warm-upâ€ pack</strong> (the bias correction) so it can start at full speed from iteration 1.</li> </ul> <p>It solves every major problem weâ€™ve identified. Itâ€™s fast, itâ€™s robust, it handles initialization, and it doesnâ€™t need nearly as much â€œlearning rateâ€ tuning.</p> <p>This is, by far, the most common optimizer youâ€™ll see.</p> <p><strong>â€¦But is it perfect?</strong></p> <p>If Adam is so great, why do many state-of-the-art research papers <em>still</em> use plain SGD with Momentum?</p> <p>It turns out, thereâ€™s a â€œLevel 3â€ debate. Some researchers believe that Adamâ€™s adaptive nature, while fast, can sometimes â€œfindâ€ a <em>worse</em> minimumâ€”one that is sharp and doesnâ€™t generalize as well as the wide, flat valleys that the â€œdumberâ€ SGD + Momentum eventually stumbles into.</p> <p>But for 99% of engineers, Adam is the robust, reliable, and powerful tool that gets the job done.</p>]]></content><author><name></name></author><category term="mlsys,"/><category term="deep-learning"/><category term="deep-learning,"/><category term="mlsys,"/><category term="optimization,"/><category term="adam,"/><category term="sgd,"/><category term="gradient-descent,"/><category term="machine-learning"/><summary type="html"><![CDATA[A deep-dive into deep learning optimizers where we trace the evolution from SGD to Momentum and finally to Adam.]]></summary></entry><entry><title type="html">How to Build a Multi-Dimensional Array by Separating View Logic from Compute Kernels</title><link href="https://minhdang26403.github.io/blog/2025/ndarray-design/" rel="alternate" type="text/html" title="How to Build a Multi-Dimensional Array by Separating View Logic from Compute Kernels"/><published>2025-11-11T11:30:00+00:00</published><updated>2025-11-11T11:30:00+00:00</updated><id>https://minhdang26403.github.io/blog/2025/ndarray-design</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2025/ndarray-design/"><![CDATA[<h3 id="the-central-design-problem">The Central Design Problem</h3> <p>When you build a tensor library, you immediately hit a fundamental fork in the road: how â€œsmartâ€ should your main <code class="language-plaintext highlighter-rouge">NDArray</code> (multi-dimensional array) object be?</p> <p>On one side, you have the â€œthinâ€ wrapper. This approach is tempting. The <code class="language-plaintext highlighter-rouge">NDArray</code> class is just a simple shell that delegates <em>everything</em>â€”all the math, all the striding logic, all the broadcastingâ€”to its backend. This sounds clean, but itâ€™s a maintenance nightmare. It means you have to re-implement all that complex, error-prone view logic in C++, then <em>again</em> in CUDA, and <em>again</em> in the language of your favoriate hardware accelerator. This design doesnâ€™t scale.</p> <p>Then, thereâ€™s the â€œthickâ€ wrapper. This is the design Iâ€™m building, and itâ€™s built on a clean separation of concerns. I call it the <strong>â€œControl Plane vs. Data Planeâ€</strong> model.</p> <p>Itâ€™s a â€œPython Brains, C++ Brawnâ€ approach:</p> <ul> <li><strong>The Control Plane (The â€œBrainsâ€):</strong> A â€œsmart,â€ â€œthickâ€ <code class="language-plaintext highlighter-rouge">NDArray</code> class written in pure Python that handles all the complex logical operations.</li> <li><strong>The Data Plane (The â€œBrawnâ€):</strong> A set of â€œdumb,â€ simple, and brutally fast compute backends (NumPy, C++, CUDA) that just do the number crunching.</li> </ul> <h3 id="ï¸-the-control-plane-a-smart-python-wrapper">âœˆï¸ The Control Plane: A â€œSmartâ€ Python Wrapper</h3> <p>The â€œControl Planeâ€ is my Python <code class="language-plaintext highlighter-rouge">NDArray</code> class. Itâ€™s â€œthickâ€ because it handles <strong>all the logical operations</strong> of the array.</p> <p>At its core, my <code class="language-plaintext highlighter-rouge">NDArray</code> is just a map. Itâ€™s a Python object that holds metadataâ€”<code class="language-plaintext highlighter-rouge">shape</code>, <code class="language-plaintext highlighter-rouge">strides</code>, and <code class="language-plaintext highlighter-rouge">offset</code>â€”which defines a â€œlogical viewâ€ over a â€œphysicalâ€ block of memory (which I call the <code class="language-plaintext highlighter-rouge">_handle</code>).</p> <p>The key insight is that <strong>a huge number of array operations are just math on this metadata.</strong> They donâ€™t need to touch the data at all, making them zero-copy, free operations.</p> <p>Take <code class="language-plaintext highlighter-rouge">permute</code>. Itâ€™s the perfect example of a â€œfreeâ€ operation that just shuffles tuples and returns a new <code class="language-plaintext highlighter-rouge">NDArray</code> pointing to the <em>exact same</em> data.</p> <p><strong>Code Snippet 1: <code class="language-plaintext highlighter-rouge">permute</code> (The â€œFreeâ€ View Operation)</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">permute</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">new_axes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="p">...],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">NDArray</span><span class="sh">"</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Permute dimensions... (no data copy).</span><span class="sh">"""</span>

        <span class="c1"># 1. Just calculate new logical shape and strides
</span>        <span class="n">new_shape</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">new_axes</span><span class="p">)</span>
        <span class="n">new_strides</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">strides</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">new_axes</span><span class="p">)</span>

        <span class="c1"># 2. Return a NEW array view, pointing to the SAME data handle
</span>        <span class="k">return</span> <span class="n">NDArray</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span>
            <span class="n">new_shape</span><span class="p">,</span>
            <span class="n">new_strides</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">_handle</span><span class="p">,</span>  <span class="c1"># &lt;-- Pass the *original* handle
</span>            <span class="n">self</span><span class="p">.</span><span class="n">_offset</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></div> <p>The Control Plane is where all this â€œviewâ€ magic lives. Slicing (<code class="language-plaintext highlighter-rouge">__getitem__</code>)? Thatâ€™s just a math problem to find a new <code class="language-plaintext highlighter-rouge">offset</code> and <code class="language-plaintext highlighter-rouge">strides</code>. Broadcasting (<code class="language-plaintext highlighter-rouge">broadcast_to</code>)? Thatâ€™s the â€œzero-stride trick.â€ Itâ€™s all handled once, in pure Python, and itâ€™s easy to test. Since the <code class="language-plaintext highlighter-rouge">__getitem__</code> and <code class="language-plaintext highlighter-rouge">broadcast_to</code> methods are a little bit more complicated than <code class="language-plaintext highlighter-rouge">permute</code>, I donâ€™t include them here, but you can take a look at their implementation at my <a href="http://github.com/minhdang26403/needle">repo</a>.</p> <h3 id="ï¸-the-data-plane-a-dumb-c-engine">âš™ï¸ The Data Plane: A â€œDumbâ€ C++ Engine</h3> <p>The â€œData Planeâ€ is my C++/CUDA backend. It is designed to be <strong>brutally fast and incredibly simple.</strong></p> <p>For this project, Iâ€™m building three backends that all follow the same simple API:</p> <ol> <li><strong>NumPy Backend:</strong> A pure Python backend using <code class="language-plaintext highlighter-rouge">numpy</code> for reference and testing.</li> <li><strong>C++ CPU Backend:</strong> A high-performance backend using plain C++ for CPU-bound computation.</li> <li><strong>CUDA Backend:</strong> A GPU-accelerated backend for high-performance training.</li> </ol> <p>My C++ and CUDA backends donâ€™t know what â€œstridesâ€ or â€œbroadcastingâ€ are. They are â€œdumbâ€ compute engines that expect one thing: <strong>a flat, 1D, contiguous block of memory.</strong></p> <p>The entire â€œdata planeâ€ API is just a set of C-style functions that operate on these flat buffers. Look at the NumPy backendâ€™s <code class="language-plaintext highlighter-rouge">reduce_sum</code> (the C++ version is nearly identical in concept):</p> <p><strong>Code Snippet 2: <code class="language-plaintext highlighter-rouge">reduce_sum</code> (The â€œDumbâ€ Backend Kernel)</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From python/needle/backends/ndarray_backend_numpy.py
</span>
<span class="k">def</span> <span class="nf">reduce_sum</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">reduce_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Reduce the last logical dimension by sum.</span><span class="sh">"""</span>

    <span class="c1"># This kernel knows nothing about 3D shapes or axes.
</span>    <span class="c1"># It just gets a flat array 'a', a 'reduce_size',
</span>    <span class="c1"># and does its job.
</span>    <span class="n">out</span><span class="p">.</span><span class="nb">buffer</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">reduce_size</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>This is the beauty of the design. The kernel is simple. It only knows how to sum â€œblocksâ€ of data. This makes my C++ and CUDA code <em>radically</em> simpler and easier to optimize.</p> <hr/> <h3 id="-the-bridge-the-compact-function">ğŸŒ‰ The Bridge: The <code class="language-plaintext highlighter-rouge">compact()</code> Function</h3> <p>This leads to the obvious question: what happens when the â€œsmartâ€ Control Plane (with its fragmented, non-contiguous <code class="language-plaintext highlighter-rouge">permute</code> view) needs to talk to the â€œdumbâ€ Data Plane (which needs a flat array)?</p> <p>This is where the most important function in the library comes in: <code class="language-plaintext highlighter-rouge">compact()</code>.</p> <p>I call <code class="language-plaintext highlighter-rouge">compact()</code> the <strong>â€œView Tax.â€</strong></p> <p>Itâ€™s the (necessary) performance hit you take to connect the two planes. When I call <code class="language-plaintext highlighter-rouge">a.sum(axis=1)</code>, the <code class="language-plaintext highlighter-rouge">sum</code> method acts as the â€œmanagerâ€ that orchestrates this whole process.</p> <p><strong>Code Snippet 3: <code class="language-plaintext highlighter-rouge">sum()</code> (The â€œManagerâ€ Connecting the Planes)</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># The "Control Plane" sum method
</span>    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">NDArray</span><span class="sh">"</span><span class="p">:</span>

        <span class="c1"># 1. Prepare the view: "permute-to-the-end" trick
</span>        <span class="n">view</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">reduce_size</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reduce_view_out</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>

        <span class="c1"># 2. Pay the "View Tax"
</span>        <span class="c1">#    This copies the fragmented 'view' into a new,
</span>        <span class="c1">#    flat, contiguous buffer.
</span>        <span class="n">compact_view_handle</span> <span class="o">=</span> <span class="n">view</span><span class="p">.</span><span class="nf">compact</span><span class="p">().</span><span class="n">_handle</span>

        <span class="c1"># 3. Call the "Dumb" Data Plane
</span>        <span class="c1">#    The backend kernel gets the simple, flat buffer it expects.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">compact_view_handle</span><span class="p">,</span> <span class="n">out</span><span class="p">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">reduce_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p>This is the whole philosophy in one method. The <code class="language-plaintext highlighter-rouge">sum</code> method calls <code class="language-plaintext highlighter-rouge">reduce_view_out</code> to get a â€œsmartâ€ permuted view, pays the <code class="language-plaintext highlighter-rouge">compact()</code> tax to â€œdefragmentâ€ it, and then hands the resulting simple, flat buffer to the â€œdumbâ€ <code class="language-plaintext highlighter-rouge">reduce_sum</code> kernel.</p> <p>As I wrote in my original notes:</p> <blockquote> <p>We call <code class="language-plaintext highlighter-rouge">.compact()</code> (which copies memory) liberally in order to make the underlying C++ implementations simpler. Everything that can be done in Python, is done in Python.</p> </blockquote> <p>This trade-offâ€”accepting a few explicit data-copy â€œtaxesâ€ in exchange for a massive reduction in backend complexityâ€”is the key to building a system that is sane, maintainable, and scalable.</p>]]></content><author><name></name></author><category term="needle"/><category term="mlsys,"/><category term="systems-design,"/><category term="python"/><summary type="html"><![CDATA[A deep dive into my multi-dimensional design. I explain the "Control Plane vs. Data Plane" model, which implements "smart" Python to handle all the complex view and stride logic, letting us build a "dumb," simple, and incredibly fast C++ and CUDA backend]]></summary></entry><entry><title type="html">Why Not Newton? The Real Reason Weâ€™re Stuck with SGD for Deep Learning</title><link href="https://minhdang26403.github.io/blog/2025/newton-vs-sgd/" rel="alternate" type="text/html" title="Why Not Newton? The Real Reason Weâ€™re Stuck with SGD for Deep Learning"/><published>2025-10-20T20:30:00+00:00</published><updated>2025-10-20T20:30:00+00:00</updated><id>https://minhdang26403.github.io/blog/2025/newton-vs-sgd</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2025/newton-vs-sgd/"><![CDATA[<p>If you come from a classic optimization background, one of the first things you ask in deep learning is, â€œWhy are we using this slow, simple algorithm called Stochastic Gradient Descent (SGD)?â€</p> <p>In a university course, you learn that Newtonâ€™s method is the king. Itâ€™s a second-order method that converges quadraticallyâ€”meaning the number of correct digits in your answerÂ <em>doubles</em>Â at every step. SGD, a first-order method, just plods along with sub-linear convergence.</p> <p>So, why are we using a horse-and-buggy when a spaceship is available?</p> <p>Itâ€™s not just one reason. Itâ€™s a three-level cascade of â€œitâ€™s a bad idea,â€ ranging from the physically impossible to the philosophically wrong.</p> <h3 id="the-level-1-answer-the-obvious-dealbreaker">The â€œLevel 1â€ Answer: The Obvious Dealbreaker</h3> <p>This is the one everyone learns first. Using Newtonâ€™s method requires computing and inverting theÂ <strong>Hessian matrix</strong>Â ($H$), which is theÂ $N \times N$Â matrix of all possible second-order partial derivatives.</p> <p>$N$Â is the number of parameters in your model.</p> <p>Letâ€™s do the math on a â€œsmallâ€ model like ResNet-50, which has aboutÂ $N = 25 \text{ million}$Â parameters.</p> <ul> <li><strong>To store the Hessian:</strong>Â You would needÂ $N \times N = (25,000,000)^2 = 625 \text{ trillion}$Â entries.</li> <li>At 32-bit (4 bytes) per entry, thatâ€™sÂ <strong>2.5 Petabytes</strong>.</li> </ul> <p>Your top-of-the-line H100 GPU has 80 gigabytes of memory. You would need around 312,500 GPUs just toÂ <em>store</em> this matrix, let alone run theÂ $O(N^3)$Â operation toÂ <em>invert</em>Â it.</p> <p>This is a full-stop, hard-no. Itâ€™s physically impossible.</p> <h3 id="the-level-2-answer-the-real-reason-even-if-we-had-the-memory">The â€œLevel 2â€ Answer: The Real Reason (Even if We Had the Memory)</h3> <p>This is where it gets interesting. A smart person will say, â€œOkay, but we haveÂ <strong>Quasi-Newton</strong>Â methods likeÂ <strong>L-BFGS</strong>. Theyâ€™re brilliant! TheyÂ <em>approximate</em>Â the inverse Hessian using just the lastÂ $k$Â gradient updates, so the memory cost is onlyÂ $O(N \cdot k)$, notÂ $O(N^2)$. Problem solved!â€</p> <p>So why donâ€™t we use L-BFGS?</p> <p>The real reason is that we doÂ <strong>stochastic</strong>Â optimization.</p> <p>L-BFGS and other classic methods are forÂ <strong>deterministic</strong>Â optimization. They assume you are calculating theÂ <em>true</em>Â gradient andÂ <em>true</em>Â Hessian over theÂ <em>entire</em>Â dataset.</p> <p>We donâ€™t do that. We use a tinyÂ <strong>mini-batch</strong>Â (e.g., 64 samples) to get aÂ <em>noisy guess</em>Â of the gradient.</p> <p>And this is the core of the problem:</p> <ol> <li><strong>A Stochastic Hessian is Garbage:</strong>Â The first derivative (gradient) is noisy but stable. If you average the â€œdownhillâ€ direction from 64 random samples, you get a pretty good estimate of the true â€œdownhill.â€ But theÂ <em>second</em>Â derivative (curvature) isÂ <em>incredibly</em>Â sensitive to noise. The curvature from a tiny mini-batch is a wildly unstable, garbage approximation of the true landscapeâ€™s curvature. Taking a â€œperfectâ€ Newton step based on this garbage map is oftenÂ <em>worse</em>Â than just taking a dumb SGD step.</li> <li><strong>The Prize (Quadratic Convergence) is a Lie:</strong>Â Even if youÂ <em>could</em>Â get a good mini-batch Hessian, the very act of using mini-batches (stochasticity) introduces noise thatÂ <em>dominates</em>Â the convergence. The best-case speed forÂ <em>any</em>Â stochastic method is sub-linear. You simplyÂ <em>cannot</em>Â achieve quadratic convergence.</li> </ol> <p>So, youâ€™d be paying the massive computational cost of a Quasi-Newton methodâ€¦ without even getting the prize.</p> <h3 id="the-level-3-answer-the-bug-is-actually-a-feature">The â€œLevel 3â€ Answer: The Bug is Actually a Feature</h3> <p>This is the final nail in the coffin. Letâ€™s say we solved the first two problems. Itâ€™sÂ <em>still</em>Â a bad idea.</p> <ul> <li><strong>Classic ML (like SVMs)</strong>Â areÂ <strong>convex</strong>Â problems. The loss landscape is a single, perfect bowl. YouÂ <em>want</em>Â a high-precision tool like Newtonâ€™s to find the one, true global minimum at the bottom.</li> <li><strong>Deep Learning</strong>Â isÂ <strong>wildly non-convex</strong>. The landscape is a hellish, high-dimensional mountain range with millions of bad local minima, saddle points, and sharp, narrow â€œoverfittingâ€ valleys.</li> </ul> <p>We donâ€™tÂ <em>want</em>Â to find the â€œsharpestâ€ minimum. A model that perfectly memorizes the training data (overfits) lives in a very sharp, deep, narrow valley.</p> <p>We want aÂ <strong>wide, flat valley</strong>. Why? Because a wide valley means that if the test data is a little different from the training data, the loss is still low. This isÂ <strong>generalization</strong>.</p> <p>This is where SGDâ€™s â€œbugâ€ becomes its greatest feature:</p> <ul> <li>TheÂ <strong>noise</strong>Â in the stochastic gradient acts as a regularizer.</li> <li>It bounces the optimizer out of those sharp, overfitted valleys.</li> <li>Itâ€™sÂ <em>too dumb and noisy</em>Â to settle into a bad, sharp minimum and prefers to find the wide, flat, generalizable ones.</li> </ul> <h3 id="the-takeaway">The Takeaway</h3> <p>So, why do we use SGD and not Newton?</p> <ol> <li><strong>Itâ€™s computationally impossible:</strong>Â TheÂ $O(N^2)$Â Hessian is petabytes large.</li> <li><strong>The â€œfixâ€ (Quasi-Newton) doesnâ€™t work:</strong>Â The stochastic Hessian is too noisy to trust, and the stochastic noise makes the prize of quadratic convergence impossible anyway.</li> <li><strong>Itâ€™s the wrong tool for the job:</strong>Â Weâ€™re in a non-convex world where the â€œdumbâ€ noise of SGD is aÂ <em>feature</em> that helps us find generalizable solutions.</li> </ol> <p>We donâ€™t want a scalpel. We need a jackhammer. And thatâ€™s SGD.</p> <p><em>(P.S. - So what is Adam? Adam is just a very, very clever jackhammer. Itâ€™s still a first-order method, but it uses momentum and an adaptive step size to navigate the landscape more intelligently than plain SGD. Itâ€™s a â€œfakeâ€ second-order method that uses first-order information to get aÂ tiny, cheapÂ approximation of the curvature. But thatâ€™s a post for another day.)</em></p>]]></content><author><name></name></author><category term="deep-learning"/><category term="machine-learning,"/><category term="deep-learning"/><summary type="html"><![CDATA[It's not just the petabyte-sized Hessian. The real reason we don't use Newton's.]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://minhdang26403.github.io/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://minhdang26403.github.io/blog/2025/plotly</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://minhdang26403.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://minhdang26403.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://minhdang26403.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://minhdang26403.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 Weâ€™re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, weâ€™re introducing Gemini 1.5 Flash: a model thatâ€™s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.Weâ€™re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5â€™s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. Itâ€™s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While itâ€™s a lighter weight model than 1.5 Pro, itâ€™s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because itâ€™s been trained by 1.5 Pro through a process called â€œdistillation,â€ where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flashâ€™s availability and pricing.Over the last few months, weâ€™ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, weâ€™ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. Weâ€™ve improved control over the modelâ€™s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And weâ€™ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And weâ€™re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do â€” not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, weâ€™re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.Weâ€™re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And weâ€™ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMindâ€™s mission to build AI responsibly to benefit humanity, weâ€™ve always wanted to develop universal AI agents that can be helpful in everyday life. Thatâ€™s why today, weâ€™re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do â€” and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While weâ€™ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, weâ€™ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, weâ€™ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context theyâ€™re being used in, and respond quickly, in conversation.With technology like this, itâ€™s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.Weâ€™ve made incredible progress so far with our family of Gemini models, and weâ€™re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, weâ€™re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Googleâ€™s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Letâ€™s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Weâ€™re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://minhdang26403.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://minhdang26403.github.io/blog/2024/tabs</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="82501a19-22ee-4ba4-9f93-85ef8b17dbfe" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="82501a19-22ee-4ba4-9f93-85ef8b17dbfe" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="5e9d0039-9116-4b32-9b43-605857a78929" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="5e9d0039-9116-4b32-9b43-605857a78929" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="87c65d09-0e83-4379-a953-d6dd349c5056" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="87c65d09-0e83-4379-a953-d6dd349c5056" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://minhdang26403.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://minhdang26403.github.io/blog/2024/typograms</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://minhdang26403.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://minhdang26403.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://minhdang26403.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://minhdang26403.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://minhdang26403.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry></feed>